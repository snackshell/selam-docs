# Vision

Certain SelamAPI models, like `selam-plus`, have vision capabilities, allowing them to understand and analyze images. You can provide images to the model along with a text prompt to perform tasks like image description, object detection, and more.

## Usage

To use the vision capabilities, you need to pass a special message format that includes both text and image URLs. The model will then process both inputs to generate a response.

Here is an example of how to analyze an image using the Python SDK:

```python copy
# Image analysis with vision-enabled models
from openai import OpenAI

client = OpenAI(
    api_key="sk-selam-your-api-key-here",
    base_url="https://api.selamapi.com/v1"
)

completion = client.chat.completions.create(
    model="selam-plus",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "What's in this image? Provide a detailed analysis."
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/1280px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
                    }
                }
            ]
        }
    ],
    max_tokens=1000
)

print(completion.choices[0].message.content)
```

Note that not all models support vision capabilities. Please refer to the [Models](/api-reference/models) page for more details on which models are vision-enabled.
